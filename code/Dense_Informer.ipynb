{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Set import"
      ],
      "metadata": {
        "id": "g9qABe9TXPw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.tseries import offsets\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "from math import sqrt\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "id": "TDd3McH5XQA_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils import"
      ],
      "metadata": {
        "id": "2Jbb0ZUHXQPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "\n",
        "class TriangularCausalMask():\n",
        "    def __init__(self, B, L, device=\"cpu\"):\n",
        "        mask_shape = [B, 1, L, L]\n",
        "        with torch.no_grad():\n",
        "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self._mask\n",
        "\n",
        "class ProbMask():\n",
        "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
        "        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
        "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
        "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
        "                             torch.arange(H)[None, :, None],\n",
        "                             index, :].to(device)\n",
        "        self._mask = indicator.view(scores.shape).to(device)\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self._mask"
      ],
      "metadata": {
        "id": "m-xfKcKYXQZN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "229wFw6OXQhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RSE(pred, true):\n",
        "    return np.sqrt(np.sum((true-pred)**2)) / np.sqrt(np.sum((true-true.mean())**2))\n",
        "\n",
        "def CORR(pred, true):\n",
        "    u = ((true-true.mean(0))*(pred-pred.mean(0))).sum(0)\n",
        "    d = np.sqrt(((true-true.mean(0))**2*(pred-pred.mean(0))**2).sum(0))\n",
        "    return (u/d).mean(-1)\n",
        "\n",
        "def MAE(pred, true):\n",
        "    return np.mean(np.abs(pred-true))\n",
        "\n",
        "def MSE(pred, true):\n",
        "    return np.mean((pred-true)**2)\n",
        "\n",
        "def RMSE(pred, true):\n",
        "    return np.sqrt(MSE(pred, true))\n",
        "\n",
        "def MAPE(pred, true):\n",
        "    return np.mean(np.abs((pred - true) / true))\n",
        "\n",
        "def MSPE(pred, true):\n",
        "    return np.mean(np.square((pred - true) / true))\n",
        "\n",
        "def metric(pred, true):\n",
        "    mae = MAE(pred, true)\n",
        "    mse = MSE(pred, true)\n",
        "    rmse = RMSE(pred, true)\n",
        "    mape = MAPE(pred, true)\n",
        "    mspe = MSPE(pred, true)\n",
        "\n",
        "    return mae,mse,rmse,mape,mspe\n",
        "\n",
        "def R2(pred, true):\n",
        "    return 1 - (np.mean((pred-true)**2) / np.mean((true-true.mean(0))**2))\n",
        "\n",
        "def metric2(pred, true):\n",
        "    r2  = R2(pred, true)\n",
        "    return r2\n",
        "\n",
        "def Accuracy(pred, true):\n",
        "    return accuracy_score(true, pred)\n",
        "def metric3(pred, true):\n",
        "    accuracy  = Accuracy(pred, true)\n",
        "    return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "tHFczfS3XQpm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "timefeatures"
      ],
      "metadata": {
        "id": "rFsr5eoZXQwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from typing import List\n",
        "\n",
        "#import numpy as np\n",
        "#import pandas as pd\n",
        "#from pandas.tseries import offsets\n",
        "#from pandas.tseries.frequencies import to_offset\n",
        "\n",
        "class TimeFeature:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"()\"\n",
        "\n",
        "class SecondOfMinute(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.second / 59.0 - 0.5\n",
        "\n",
        "class MinuteOfHour(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.minute / 59.0 - 0.5\n",
        "\n",
        "class HourOfDay(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.hour / 23.0 - 0.5\n",
        "\n",
        "class DayOfWeek(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.dayofweek / 6.0 - 0.5\n",
        "\n",
        "class DayOfMonth(TimeFeature):\n",
        "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.day - 1) / 30.0 - 0.5\n",
        "\n",
        "class DayOfYear(TimeFeature):\n",
        "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
        "\n",
        "class MonthOfYear(TimeFeature):\n",
        "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.month - 1) / 11.0 - 0.5\n",
        "\n",
        "class WeekOfYear(TimeFeature):\n",
        "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.week - 1) / 52.0 - 0.5\n",
        "\n",
        "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
        "    \"\"\"\n",
        "    Returns a list of time features that will be appropriate for the given frequency string.\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_str\n",
        "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
        "    \"\"\"\n",
        "\n",
        "    features_by_offsets = {\n",
        "        offsets.YearEnd: [],\n",
        "        offsets.QuarterEnd: [MonthOfYear],\n",
        "        offsets.MonthEnd: [MonthOfYear],\n",
        "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
        "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Minute: [\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "        offsets.Second: [\n",
        "            SecondOfMinute,\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    offset = to_offset(freq_str)\n",
        "\n",
        "    for offset_type, feature_classes in features_by_offsets.items():\n",
        "        if isinstance(offset, offset_type):\n",
        "            return [cls() for cls in feature_classes]\n",
        "\n",
        "    supported_freq_msg = f\"\"\"\n",
        "    Unsupported frequency {freq_str}\n",
        "    The following frequencies are supported:\n",
        "        Y   - yearly\n",
        "            alias: A\n",
        "        M   - monthly\n",
        "        W   - weekly\n",
        "        D   - daily\n",
        "        B   - business days\n",
        "        H   - hourly\n",
        "        T   - minutely\n",
        "            alias: min\n",
        "        S   - secondly\n",
        "    \"\"\"\n",
        "    raise RuntimeError(supported_freq_msg)\n",
        "\n",
        "def time_features(dates, timeenc=1, freq='h'):\n",
        "    \"\"\"\n",
        "    > `time_features` takes in a `dates` dataframe with a 'dates' column and extracts the date down to `freq` where freq can be any of the following if `timeenc` is 0:\n",
        "    > * m - [month]\n",
        "    > * w - [month]\n",
        "    > * d - [month, day, weekday]\n",
        "    > * b - [month, day, weekday]\n",
        "    > * h - [month, day, weekday, hour]\n",
        "    > * t - [month, day, weekday, hour, *minute]\n",
        "    >\n",
        "    > If `timeenc` is 1, a similar, but different list of `freq` values are supported (all encoded between [-0.5 and 0.5]):\n",
        "    > * Q - [month]\n",
        "    > * M - [month]\n",
        "    > * W - [Day of month, week of year]\n",
        "    > * D - [Day of week, day of month, day of year]\n",
        "    > * B - [Day of week, day of month, day of year]\n",
        "    > * H - [Hour of day, day of week, day of month, day of year]\n",
        "    > * T - [Minute of hour*, hour of day, day of week, day of month, day of year]\n",
        "    > * S - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year]\n",
        "\n",
        "    *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.\n",
        "    \"\"\"\n",
        "    if timeenc==0:\n",
        "        dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
        "        dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
        "        dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
        "        dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
        "        dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
        "        dates['minute'] = dates.minute.map(lambda x:x//15)\n",
        "        freq_map = {\n",
        "            'y':[],'m':['month'],'w':['month'],'d':['month','day','weekday'],\n",
        "            'b':['month','day','weekday'],'h':['month','day','weekday','hour'],\n",
        "            't':['month','day','weekday','hour','minute'],\n",
        "        }\n",
        "        return dates[freq_map[freq.lower()]].values\n",
        "    if timeenc==1:\n",
        "        dates = pd.to_datetime(dates.date.values)\n",
        "        return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)]).transpose(1,0)\n"
      ],
      "metadata": {
        "id": "k64NO1gRXQ32"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tools"
      ],
      "metadata": {
        "id": "tym7mUwLXQ--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "#import torch\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
        "    if args.lradj=='type1':\n",
        "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch-1) // 1))}\n",
        "    elif args.lradj=='type2':\n",
        "        lr_adjust = {\n",
        "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
        "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
        "        }\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model, path):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), path+'/'+'checkpoint.pth')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "class StandardScaler():\n",
        "    def __init__(self):\n",
        "        self.mean = 0.\n",
        "        self.std = 1.\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.mean = data.mean(0)\n",
        "        self.std = data.std(0)\n",
        "\n",
        "    def transform(self, data):\n",
        "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
        "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
        "        return (data - mean) / std\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
        "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
        "        if data.shape[-1] != mean.shape[-1]:\n",
        "            mean = mean[-1:]\n",
        "            std = std[-1:]\n",
        "        return (data * std) + mean\n"
      ],
      "metadata": {
        "id": "6RUbYEelXRGW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "XS09Hjl-XYUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#import numpy as np\n",
        "#import pandas as pd\n",
        "\n",
        "#import torch\n",
        "#from torch.utils.data import Dataset, DataLoader\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#from utils.tools import StandardScaler  #這可以不用\n",
        "#from utils.timefeatures import time_features  #這可以不用\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class Dataset_ETT_hour(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, inverse=False, timeenc=0, freq='h', cols=None):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24*4*4\n",
        "            self.label_len = 24*4\n",
        "            self.pred_len = 24*4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train':0, 'val':1, 'test':2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12*30*24 - self.seq_len, 12*30*24+4*30*24 - self.seq_len]\n",
        "        border2s = [12*30*24, 12*30*24+4*30*24, 12*30*24+8*30*24]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features=='M' or self.features=='MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features=='S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = np.concatenate([self.data_x[r_begin:r_begin+self.label_len], self.data_y[r_begin+self.label_len:r_end]], 0)\n",
        "        else:\n",
        "            seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len- self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "class Dataset_ETT_minute(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTm1.csv',\n",
        "                 target='OT', scale=True, inverse=False, timeenc=0, freq='t', cols=None):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24*4*4\n",
        "            self.label_len = 24*4\n",
        "            self.pred_len = 24*4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train':0, 'val':1, 'test':2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12*30*24*4 - self.seq_len, 12*30*24*4+4*30*24*4 - self.seq_len]\n",
        "        border2s = [12*30*24*4, 12*30*24*4+4*30*24*4, 12*30*24*4+8*30*24*4]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features=='M' or self.features=='MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features=='S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = np.concatenate([self.data_x[r_begin:r_begin+self.label_len], self.data_y[r_begin+self.label_len:r_end]], 0)\n",
        "        else:\n",
        "            seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "\n",
        "class Dataset_Custom(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, inverse=False, timeenc=0, freq='h', cols=None):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24*4*4\n",
        "            self.label_len = 24*4\n",
        "            self.pred_len = 24*4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train':0, 'val':1, 'test':2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.cols=cols\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "        '''\n",
        "        df_raw.columns: ['date', ...(other features), target feature]\n",
        "        '''\n",
        "        # cols = list(df_raw.columns);\n",
        "        if self.cols:\n",
        "            cols=self.cols.copy()\n",
        "            cols.remove(self.target)\n",
        "        else:\n",
        "            cols = list(df_raw.columns); cols.remove(self.target); cols.remove('date')\n",
        "        df_raw = df_raw[['date']+cols+[self.target]]\n",
        "\n",
        "        num_train = int(len(df_raw)*0.7)\n",
        "        num_test = int(len(df_raw)*0.2)\n",
        "        num_vali = len(df_raw) - num_train - num_test\n",
        "        border1s = [0, num_train-self.seq_len, len(df_raw)-num_test-self.seq_len]\n",
        "        border2s = [num_train, num_train+num_vali, len(df_raw)]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features=='M' or self.features=='MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features=='S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = np.concatenate([self.data_x[r_begin:r_begin+self.label_len], self.data_y[r_begin+self.label_len:r_end]], 0)\n",
        "        else:\n",
        "            seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len- self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "class Dataset_Pred(Dataset):\n",
        "    def __init__(self, root_path, flag='pred', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, inverse=False, timeenc=0, freq='15min', cols=None):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24*4*4\n",
        "            self.label_len = 24*4\n",
        "            self.pred_len = 24*4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['pred']\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.cols=cols\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "        '''\n",
        "        df_raw.columns: ['date', ...(other features), target feature]\n",
        "        '''\n",
        "        if self.cols:\n",
        "            cols=self.cols.copy()\n",
        "            cols.remove(self.target)\n",
        "        else:\n",
        "            cols = list(df_raw.columns); cols.remove(self.target); cols.remove('date')\n",
        "        df_raw = df_raw[['date']+cols+[self.target]]\n",
        "\n",
        "        border1 = len(df_raw)-self.seq_len\n",
        "        border2 = len(df_raw)\n",
        "\n",
        "        if self.features=='M' or self.features=='MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features=='S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            self.scaler.fit(df_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        tmp_stamp = df_raw[['date']][border1:border2]\n",
        "        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)\n",
        "        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len+1, freq=self.freq)\n",
        "\n",
        "        df_stamp = pd.DataFrame(columns = ['date'])\n",
        "        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])\n",
        "        data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq[-1:])\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = self.data_x[r_begin:r_begin+self.label_len]\n",
        "        else:\n",
        "            seq_y = self.data_y[r_begin:r_begin+self.label_len]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n"
      ],
      "metadata": {
        "id": "RBD7TExZXYq4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "attnention set"
      ],
      "metadata": {
        "id": "STzL9_fnXY1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "#import numpy as np\n",
        "\n",
        "#from math import sqrt\n",
        "#from utils.masking import TriangularCausalMask, ProbMask\n",
        "\n",
        "class FullAttention(nn.Module):\n",
        "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
        "        super(FullAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "        scale = self.scale or 1./sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
        "        if self.mask_flag:\n",
        "            if attn_mask is None:\n",
        "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
        "\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "\n",
        "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
        "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return (V.contiguous(), A)\n",
        "        else:\n",
        "            return (V.contiguous(), None)\n",
        "\n",
        "class ProbAttention(nn.Module):\n",
        "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
        "        super(ProbAttention, self).__init__()\n",
        "        self.factor = factor\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def _prob_QK(self, Q, K, sample_k, n_top): # n_top: c*ln(L_q)\n",
        "        # Q [B, H, L, D]\n",
        "        B, H, L_K, E = K.shape\n",
        "        _, _, L_Q, _ = Q.shape\n",
        "\n",
        "        # calculate the sampled Q_K\n",
        "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
        "        index_sample = torch.randint(L_K, (L_Q, sample_k)) # real U = U_part(factor*ln(L_k))*L_q\n",
        "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
        "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
        "\n",
        "        # find the Top_k query with sparisty measurement\n",
        "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
        "        M_top = M.topk(n_top, sorted=False)[1]\n",
        "\n",
        "        # use the reduced Q to calculate Q_K\n",
        "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
        "                     torch.arange(H)[None, :, None],\n",
        "                     M_top, :] # factor*ln(L_q)\n",
        "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1)) # factor*ln(L_q)*L_k\n",
        "\n",
        "        return Q_K, M_top\n",
        "\n",
        "    def _get_initial_context(self, V, L_Q):\n",
        "        B, H, L_V, D = V.shape\n",
        "        if not self.mask_flag:\n",
        "            # V_sum = V.sum(dim=-2)\n",
        "            V_sum = V.mean(dim=-2)\n",
        "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
        "        else: # use mask\n",
        "            assert(L_Q == L_V) # requires that L_Q == L_V, i.e. for self-attention only\n",
        "            contex = V.cumsum(dim=-2)\n",
        "        return contex\n",
        "\n",
        "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
        "        B, H, L_V, D = V.shape\n",
        "\n",
        "        if self.mask_flag:\n",
        "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n",
        "\n",
        "        context_in[torch.arange(B)[:, None, None],\n",
        "                   torch.arange(H)[None, :, None],\n",
        "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
        "        if self.output_attention:\n",
        "            attns = (torch.ones([B, H, L_V, L_V])/L_V).type_as(attn).to(attn.device)\n",
        "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
        "            return (context_in, attns)\n",
        "        else:\n",
        "            return (context_in, None)\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L_Q, H, D = queries.shape\n",
        "        _, L_K, _, _ = keys.shape\n",
        "\n",
        "        queries = queries.transpose(2,1)\n",
        "        keys = keys.transpose(2,1)\n",
        "        values = values.transpose(2,1)\n",
        "\n",
        "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item() # c*ln(L_k)\n",
        "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item() # c*ln(L_q)\n",
        "\n",
        "        U_part = U_part if U_part<L_K else L_K\n",
        "        u = u if u<L_Q else L_Q\n",
        "\n",
        "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
        "\n",
        "        # add scale factor\n",
        "        scale = self.scale or 1./sqrt(D)\n",
        "        if scale is not None:\n",
        "            scores_top = scores_top * scale\n",
        "        # get the context\n",
        "        context = self._get_initial_context(values, L_Q)\n",
        "        # update the context with selected top_k queries\n",
        "        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n",
        "\n",
        "        return context.transpose(2,1).contiguous(), attn\n",
        "\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, n_heads,\n",
        "                 d_keys=None, d_values=None, mix=False):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model//n_heads)\n",
        "        d_values = d_values or (d_model//n_heads)\n",
        "\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "        self.n_heads = n_heads\n",
        "        self.mix = mix\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "\n",
        "        out, attn = self.inner_attention(\n",
        "            queries,\n",
        "            keys,\n",
        "            values,\n",
        "            attn_mask\n",
        "        )\n",
        "        if self.mix:\n",
        "            out = out.transpose(2,1).contiguous()\n",
        "        out = out.view(B, L, -1)\n",
        "\n",
        "        return self.out_projection(out), attn\n"
      ],
      "metadata": {
        "id": "C61OUVypXY-o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decoder"
      ],
      "metadata": {
        "id": "_8U-CwE7XZHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
        "                 dropout=0.1, activation=\"relu\"):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4*d_model\n",
        "        self.self_attention = self_attention\n",
        "        self.cross_attention = cross_attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
        "        self.downConv = nn.Conv1d(in_channels=d_model,\n",
        "                                  out_channels=d_model,\n",
        "                                  kernel_size=3,\n",
        "                                  padding=padding,\n",
        "                                  padding_mode='circular')\n",
        "        self.norm = nn.BatchNorm1d(d_model)\n",
        "        self.activation = nn.ELU()\n",
        "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
        "        xo5 = x\n",
        "        x = x + self.dropout(self.self_attention(\n",
        "            x, x, x,\n",
        "            attn_mask=x_mask\n",
        "        )[0])\n",
        "        x = self.norm1(x)\n",
        "        xo5 = self.dropout(self.activation(self.conv1(xo5.transpose(-1,1))))\n",
        "        xo5 = self.dropout(self.conv2(xo5).transpose(-1,1))\n",
        "        xo5 = self.norm2(xo5)\n",
        "        x = self.dropout(self.activation(self.conv1(x.transpose(-1,1))))\n",
        "        x = self.dropout(self.conv2(x).transpose(-1,1))\n",
        "        x = self.norm2(x)\n",
        "        x = x + xo5\n",
        "        x = x + self.dropout(self.cross_attention(\n",
        "            x, cross, cross,\n",
        "            attn_mask=cross_mask\n",
        "        )[0])\n",
        "\n",
        "        y = x = self.norm2(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1,1))\n",
        "\n",
        "        return self.norm3(x+y)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers, norm_layer=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "iwTubjnpXZRA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding"
      ],
      "metadata": {
        "id": "Dgqne7zDXZaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "#import math\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
        "        return x\n",
        "\n",
        "class FixedEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(FixedEmbedding, self).__init__()\n",
        "\n",
        "        w = torch.zeros(c_in, d_model).float()\n",
        "        w.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        w[:, 0::2] = torch.sin(position * div_term)\n",
        "        w[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.emb = nn.Embedding(c_in, d_model)\n",
        "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x).detach()\n",
        "\n",
        "class TemporalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
        "        super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "        minute_size = 4; hour_size = 24\n",
        "        weekday_size = 7; day_size = 32; month_size = 13\n",
        "\n",
        "        Embed = FixedEmbedding if embed_type=='fixed' else nn.Embedding\n",
        "        if freq=='t':\n",
        "            self.minute_embed = Embed(minute_size, d_model)\n",
        "        self.hour_embed = Embed(hour_size, d_model)\n",
        "        self.weekday_embed = Embed(weekday_size, d_model)\n",
        "        self.day_embed = Embed(day_size, d_model)\n",
        "        self.month_embed = Embed(month_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "\n",
        "        minute_x = self.minute_embed(x[:,:,4]) if hasattr(self, 'minute_embed') else 0.\n",
        "        hour_x = self.hour_embed(x[:,:,3])\n",
        "        weekday_x = self.weekday_embed(x[:,:,2])\n",
        "        day_x = self.day_embed(x[:,:,1])\n",
        "        month_x = self.month_embed(x[:,:,0])\n",
        "\n",
        "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
        "\n",
        "class TimeFeatureEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
        "        super(TimeFeatureEmbedding, self).__init__()\n",
        "\n",
        "        freq_map = {'h':4, 't':5, 's':6, 'm':1, 'a':1, 'w':2, 'd':3, 'b':3}\n",
        "        d_inp = freq_map[freq]\n",
        "        self.embed = nn.Linear(d_inp, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if embed_type!='timeF' else TimeFeatureEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
        "\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "0eZVksGeXZjZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder"
      ],
      "metadata": {
        "id": "xpglRlCdXZsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, c_in):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
        "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
        "                                  out_channels=c_in,\n",
        "                                  kernel_size=3,\n",
        "                                  padding=padding,\n",
        "                                  padding_mode='circular')\n",
        "        self.norm = nn.BatchNorm1d(c_in)\n",
        "        self.activation = nn.ELU()\n",
        "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downConv(x.permute(0, 2, 1))\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.maxPool(x)\n",
        "        x = x.transpose(1,2)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4*d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
        "        self.downConv = nn.Conv1d(in_channels=d_model,\n",
        "                                  out_channels=d_model,\n",
        "                                  kernel_size=3,\n",
        "                                  padding=padding,\n",
        "                                  padding_mode='circular')\n",
        "        self.norm = nn.BatchNorm1d(d_model)\n",
        "        self.activation = nn.ELU()\n",
        "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        new_x, attn = self.attention(\n",
        "            x, x, x,\n",
        "            attn_mask = attn_mask\n",
        "        )\n",
        "        xo1 = x.shape[1]\n",
        "        if xo1 == 96:\n",
        "          global xo2\n",
        "          xo2 = x\n",
        "          xo2 = self.downConv(xo2.permute(0, 2, 1))\n",
        "          xo2 = self.norm(xo2)\n",
        "          xo2 = self.activation(xo2)\n",
        "          xo2 = self.maxPool(xo2)\n",
        "          xo2 = xo2.transpose(1,2)\n",
        "        elif xo1 == 48:\n",
        "          global xo3\n",
        "          xo3 = x\n",
        "          xo3 = self.downConv(xo3.permute(0, 2, 1))\n",
        "          xo3 = self.norm(xo3)\n",
        "          xo3 = self.activation(xo3)\n",
        "          xo3 = self.maxPool(xo3)\n",
        "          xo3 = xo3.transpose(1,2)\n",
        "          x = x + xo2\n",
        "        elif xo1 == 24:\n",
        "          global xo4\n",
        "          xo4 = x\n",
        "          xo2 = self.downConv(xo2.permute(0, 2, 1))\n",
        "          xo2 = self.norm(xo2)\n",
        "          xo2 = self.activation(xo2)\n",
        "          xo2 = self.maxPool(xo2)\n",
        "          xo2 = xo2.transpose(1,2)\n",
        "          x = x + xo2 + xo3\n",
        "        x = x + self.dropout(new_x)\n",
        "\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1,1))\n",
        "        if xo1 == 24:\n",
        "          xo2 = self.downConv(xo2.permute(0, 2, 1))\n",
        "          xo2 = self.norm(xo2)\n",
        "          xo2 = self.activation(xo2)\n",
        "          xo2 = xo2.transpose(1,2)\n",
        "          xo3 = self.downConv(xo3.permute(0, 2, 1))\n",
        "          xo3 = self.norm(xo3)\n",
        "          xo3 = self.activation(xo3)\n",
        "          xo3 = xo3.transpose(1,2)\n",
        "          xo4 = self.downConv(xo4.permute(0, 2, 1))\n",
        "          xo4 = self.norm(xo4)\n",
        "          xo4 = self.activation(xo4)\n",
        "          xo4 = xo4.transpose(1,2)\n",
        "          x = x + xo2 + xo3 + xo4\n",
        "        return self.norm2(x+y), attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        attns = []\n",
        "        if self.conv_layers is not None:\n",
        "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                x = conv_layer(x)\n",
        "                attns.append(attn)\n",
        "            x, attn = self.attn_layers[-1](x, attn_mask=attn_mask)\n",
        "            attns.append(attn)\n",
        "        else:\n",
        "            for attn_layer in self.attn_layers:\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                attns.append(attn)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x, attns\n",
        "\n",
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, encoders, inp_lens):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "        self.inp_lens = inp_lens\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        x_stack = []; attns = []\n",
        "        for i_len, encoder in zip(self.inp_lens, self.encoders):\n",
        "            inp_len = x.shape[1]//(2**i_len)\n",
        "            x_s, attn = encoder(x[:, -inp_len:, :])\n",
        "            x_stack.append(x_s); attns.append(attn)\n",
        "        x_stack = torch.cat(x_stack, -2)\n",
        "\n",
        "        return x_stack, attns\n"
      ],
      "metadata": {
        "id": "UXmYnrFmXZ1Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "modelimport torch"
      ],
      "metadata": {
        "id": "iNAlOuhzXZ9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Informer(nn.Module):\n",
        "    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,\n",
        "                factor=5, d_model=512, n_heads=8, e_layers=3, d_layers=2, d_ff=512,\n",
        "                dropout=0.0, attn='prob', embed='fixed', freq='h', activation='gelu',\n",
        "                output_attention = False, distil=True, mix=True,\n",
        "                device=torch.device('cuda:0')):\n",
        "        super(Informer, self).__init__()\n",
        "        self.pred_len = out_len\n",
        "        self.attn = attn\n",
        "        self.output_attention = output_attention\n",
        "\n",
        "        # Encoding\n",
        "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
        "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq, dropout)\n",
        "        # Attention\n",
        "        Attn = ProbAttention if attn=='prob' else FullAttention\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AttentionLayer(Attn(False, factor, attention_dropout=dropout, output_attention=output_attention),\n",
        "                                d_model, n_heads, mix=False),\n",
        "                    d_model,\n",
        "                    d_ff,\n",
        "                    dropout=dropout,\n",
        "                    activation=activation\n",
        "                ) for l in range(e_layers)\n",
        "            ],\n",
        "            [\n",
        "                ConvLayer(\n",
        "                    d_model\n",
        "                ) for l in range(e_layers-1)\n",
        "            ] if distil else None,\n",
        "            norm_layer=torch.nn.LayerNorm(d_model)\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(\n",
        "            [\n",
        "                DecoderLayer(\n",
        "                    AttentionLayer(Attn(True, factor, attention_dropout=dropout, output_attention=False),\n",
        "                                d_model, n_heads, mix=mix),\n",
        "                    AttentionLayer(FullAttention(False, factor, attention_dropout=dropout, output_attention=False),\n",
        "                                d_model, n_heads, mix=False),\n",
        "                    d_model,\n",
        "                    d_ff,\n",
        "                    dropout=dropout,\n",
        "                    activation=activation,\n",
        "                )\n",
        "                for l in range(d_layers)\n",
        "            ],\n",
        "            norm_layer=torch.nn.LayerNorm(d_model)\n",
        "        )\n",
        "        # self.end_conv1 = nn.Conv1d(in_channels=label_len+out_len, out_channels=out_len, kernel_size=1, bias=True)\n",
        "        # self.end_conv2 = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=1, bias=True)\n",
        "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
        "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
        "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
        "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
        "\n",
        "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
        "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
        "        dec_out = self.projection(dec_out)\n",
        "\n",
        "        # dec_out = self.end_conv1(dec_out)\n",
        "        # dec_out = self.end_conv2(dec_out.transpose(2,1)).transpose(1,2)\n",
        "        if self.output_attention:\n",
        "            return dec_out[:,-self.pred_len:,:], attns\n",
        "        else:\n",
        "            return dec_out[:,-self.pred_len:,:] # [B, L, D]\n",
        "\n",
        "\n",
        "class InformerStack(nn.Module):\n",
        "    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,\n",
        "                factor=5, d_model=512, n_heads=8, e_layers=[3,2,1], d_layers=2, d_ff=512,\n",
        "                dropout=0.0, attn='prob', embed='fixed', freq='h', activation='gelu',\n",
        "                output_attention = False, distil=True, mix=True,\n",
        "                device=torch.device('cuda:0')):\n",
        "        super(InformerStack, self).__init__()\n",
        "        self.pred_len = out_len\n",
        "        self.attn = attn\n",
        "        self.output_attention = output_attention\n",
        "\n",
        "        # Encoding\n",
        "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
        "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq, dropout)\n",
        "        # Attention\n",
        "        Attn = ProbAttention if attn=='prob' else FullAttention\n",
        "        # Encoder\n",
        "\n",
        "        inp_lens = list(range(len(e_layers))) # [0,1,2,...] you can customize here\n",
        "        encoders = [\n",
        "            Encoder(\n",
        "                [\n",
        "                    EncoderLayer(\n",
        "                        AttentionLayer(Attn(False, factor, attention_dropout=dropout, output_attention=output_attention),\n",
        "                                    d_model, n_heads, mix=False),\n",
        "                        d_model,\n",
        "                        d_ff,\n",
        "                        dropout=dropout,\n",
        "                        activation=activation\n",
        "                    ) for l in range(el)\n",
        "                ],\n",
        "                [\n",
        "                    ConvLayer(\n",
        "                        d_model\n",
        "                    ) for l in range(el-1)\n",
        "                ] if distil else None,\n",
        "                norm_layer=torch.nn.LayerNorm(d_model)\n",
        "            ) for el in e_layers]\n",
        "        self.encoder = EncoderStack(encoders, inp_lens)\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(\n",
        "            [\n",
        "                DecoderLayer(\n",
        "                    AttentionLayer(Attn(True, factor, attention_dropout=dropout, output_attention=False),\n",
        "                                d_model, n_heads, mix=mix),\n",
        "                    AttentionLayer(FullAttention(False, factor, attention_dropout=dropout, output_attention=False),\n",
        "                                d_model, n_heads, mix=False),\n",
        "                    d_model,\n",
        "                    d_ff,\n",
        "                    dropout=dropout,\n",
        "                    activation=activation,\n",
        "                )\n",
        "                for l in range(d_layers)\n",
        "            ],\n",
        "            norm_layer=torch.nn.LayerNorm(d_model)\n",
        "        )\n",
        "        # self.end_conv1 = nn.Conv1d(in_channels=label_len+out_len, out_channels=out_len, kernel_size=1, bias=True)\n",
        "        # self.end_conv2 = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=1, bias=True)\n",
        "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
        "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
        "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
        "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
        "\n",
        "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
        "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
        "        dec_out = self.projection(dec_out)\n",
        "\n",
        "        # dec_out = self.end_conv1(dec_out)\n",
        "        # dec_out = self.end_conv2(dec_out.transpose(2,1)).transpose(1,2)\n",
        "        if self.output_attention:\n",
        "            return dec_out[:,-self.pred_len:,:], attns\n",
        "        else:\n",
        "            return dec_out[:,-self.pred_len:,:] # [B, L, D]\n"
      ],
      "metadata": {
        "id": "v0Qy54wnXaGh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "exp import -exp_basic"
      ],
      "metadata": {
        "id": "yzoJKzBrXaO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#import torch\n",
        "#import numpy as np\n",
        "\n",
        "class Exp_Basic(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = self._acquire_device()\n",
        "        self.model = self._build_model().to(self.device)\n",
        "\n",
        "    def _build_model(self):\n",
        "        raise NotImplementedError\n",
        "        return None\n",
        "\n",
        "    def _acquire_device(self):\n",
        "        if self.args.use_gpu:\n",
        "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n",
        "            device = torch.device('cuda:{}'.format(self.args.gpu))\n",
        "            print('Use GPU: cuda:{}'.format(self.args.gpu))\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "            print('Use CPU')\n",
        "        return device\n",
        "\n",
        "    def _get_data(self):\n",
        "        pass\n",
        "\n",
        "    def vali(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "    def test(self):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "F9mCjSoKXaX4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "exp_informer"
      ],
      "metadata": {
        "id": "Pqf2s2agXaf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharbonnierLoss(nn.Module):\n",
        "    def __init__(self, eps=1e-3): #0.001\n",
        "      super(CharbonnierLoss, self).__init__()\n",
        "      self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "      diff = x - y\n",
        "      loss = torch.mean(torch.sqrt((diff * diff) + (self.eps * self.eps)))\n",
        "      return loss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class Exp_Informer(Exp_Basic):\n",
        "    def __init__(self, args):\n",
        "        super(Exp_Informer, self).__init__(args)\n",
        "\n",
        "    def _build_model(self):\n",
        "        model_dict = {\n",
        "            'informer':Informer,\n",
        "            'informerstack':InformerStack,\n",
        "        }\n",
        "        if self.args.model=='informer' or self.args.model=='informerstack':\n",
        "            e_layers = self.args.e_layers if self.args.model=='informer' else self.args.s_layers\n",
        "            model = model_dict[self.args.model](\n",
        "                self.args.enc_in,\n",
        "                self.args.dec_in,\n",
        "                self.args.c_out,\n",
        "                self.args.seq_len,\n",
        "                self.args.label_len,\n",
        "                self.args.pred_len,\n",
        "                self.args.factor,\n",
        "                self.args.d_model,\n",
        "                self.args.n_heads,\n",
        "                e_layers, # self.args.e_layers,\n",
        "                self.args.d_layers,\n",
        "                self.args.d_ff,\n",
        "                self.args.dropout,\n",
        "                self.args.attn,\n",
        "                self.args.embed,\n",
        "                self.args.freq,\n",
        "                self.args.activation,\n",
        "                self.args.output_attention,\n",
        "                self.args.distil,\n",
        "                self.args.mix,\n",
        "                self.device\n",
        "            ).float()\n",
        "\n",
        "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
        "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
        "        return model\n",
        "\n",
        "    def _get_data(self, flag):\n",
        "        args = self.args\n",
        "\n",
        "        data_dict = {\n",
        "            'ETTh1':Dataset_ETT_hour,\n",
        "            'ETTh2':Dataset_ETT_hour,\n",
        "            'ETTm1':Dataset_ETT_minute,\n",
        "            'ETTm2':Dataset_ETT_minute,\n",
        "            'WTH':Dataset_Custom,\n",
        "            'ECL':Dataset_Custom,\n",
        "            'Solar':Dataset_Custom,\n",
        "            'custom':Dataset_Custom,\n",
        "        }\n",
        "        Data = data_dict[self.args.data]\n",
        "        timeenc = 0 if args.embed!='timeF' else 1\n",
        "\n",
        "        if flag == 'test':\n",
        "            shuffle_flag = False; drop_last = True; batch_size = args.batch_size; freq=args.freq\n",
        "        elif flag=='pred':\n",
        "            shuffle_flag = False; drop_last = False; batch_size = 1; freq=args.detail_freq\n",
        "            Data = Dataset_Pred\n",
        "        else:\n",
        "            shuffle_flag = True; drop_last = True; batch_size = args.batch_size; freq=args.freq\n",
        "        data_set = Data(\n",
        "            root_path=args.root_path,\n",
        "            data_path=args.data_path,\n",
        "            flag=flag,\n",
        "            size=[args.seq_len, args.label_len, args.pred_len],\n",
        "            features=args.features,\n",
        "            target=args.target,\n",
        "            inverse=args.inverse,\n",
        "            timeenc=timeenc,\n",
        "            freq=freq,\n",
        "            cols=args.cols\n",
        "        )\n",
        "        print(flag, len(data_set))\n",
        "        data_loader = DataLoader(\n",
        "            data_set,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle_flag,\n",
        "            num_workers=args.num_workers,\n",
        "            drop_last=drop_last)\n",
        "\n",
        "        return data_set, data_loader\n",
        "\n",
        "    def _select_optimizer(self):\n",
        "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
        "        return model_optim\n",
        "\n",
        "    def _select_criterion(self):\n",
        "        mse_loss = nn.MSELoss()\n",
        "        charbonnier_loss = CharbonnierLoss()\n",
        "\n",
        "        def criterion(pred, true):\n",
        "          return 0.2 * mse_loss(pred, true) + 0.8 * charbonnier_loss(pred, true)\n",
        "\n",
        "        return criterion\n",
        "\n",
        "    def vali(self, vali_data, vali_loader, criterion):\n",
        "        self.model.eval()\n",
        "        total_loss = []\n",
        "        for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(vali_loader):\n",
        "            pred, true = self._process_one_batch(\n",
        "                vali_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
        "            loss = criterion(pred.detach().cpu(), true.detach().cpu())\n",
        "            total_loss.append(loss)\n",
        "        total_loss = np.average(total_loss)\n",
        "        self.model.train()\n",
        "        return total_loss\n",
        "\n",
        "    def train(self, setting):\n",
        "        train_data, train_loader = self._get_data(flag = 'train')\n",
        "        vali_data, vali_loader = self._get_data(flag = 'val')\n",
        "        test_data, test_loader = self._get_data(flag = 'test')\n",
        "\n",
        "        path = os.path.join(self.args.checkpoints, setting)\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        time_now = time.time()\n",
        "\n",
        "        train_steps = len(train_loader)\n",
        "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
        "\n",
        "        model_optim = self._select_optimizer()\n",
        "        criterion =  self._select_criterion()\n",
        "\n",
        "        if self.args.use_amp:\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        for epoch in range(self.args.train_epochs):\n",
        "            iter_count = 0\n",
        "            train_loss = []\n",
        "\n",
        "            self.model.train()\n",
        "            epoch_time = time.time()\n",
        "            for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(train_loader):\n",
        "                iter_count += 1\n",
        "\n",
        "                model_optim.zero_grad()\n",
        "                pred, true = self._process_one_batch(\n",
        "                    train_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
        "                loss = criterion(pred, true)\n",
        "                train_loss.append(loss.item())\n",
        "\n",
        "                if (i+1) % 100==0:\n",
        "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
        "                    speed = (time.time()-time_now)/iter_count\n",
        "                    left_time = speed*((self.args.train_epochs - epoch)*train_steps - i)\n",
        "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
        "                    iter_count = 0\n",
        "                    time_now = time.time()\n",
        "\n",
        "                if self.args.use_amp:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(model_optim)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    model_optim.step()\n",
        "\n",
        "            print(\"Epoch: {} cost time: {}\".format(epoch+1, time.time()-epoch_time))\n",
        "            train_loss = np.average(train_loss)\n",
        "            vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
        "            test_loss = self.vali(test_data, test_loader, criterion)\n",
        "\n",
        "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
        "                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
        "            early_stopping(vali_loss, self.model, path)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "            adjust_learning_rate(model_optim, epoch+1, self.args)\n",
        "\n",
        "        best_model_path = path+'/'+'checkpoint.pth'\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def test(self, setting):\n",
        "        test_data, test_loader = self._get_data(flag='test')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        preds = []\n",
        "        trues = []\n",
        "\n",
        "        for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(test_loader):\n",
        "            pred, true = self._process_one_batch(\n",
        "                test_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
        "            preds.append(pred.detach().cpu().numpy())\n",
        "            trues.append(true.detach().cpu().numpy())\n",
        "\n",
        "        preds = np.array(preds)\n",
        "        trues = np.array(trues)\n",
        "        print('test shape:', preds.shape, trues.shape)\n",
        "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
        "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
        "        print('test shape:', preds.shape, trues.shape)\n",
        "\n",
        "        # result save\n",
        "        folder_path = './results/' + setting +'/'\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "        mae, mse, rmse, mape, mspe = metric(preds, trues)\n",
        "        np.save(folder_path+'pred.npy', preds)\n",
        "        np.save(folder_path+'true.npy', trues)\n",
        "        np.save(folder_path+'metrics.npy', np.array([mae, mse, rmse, mape, mspe]))\n",
        "        preds = preds[:,:,-1]\n",
        "        trues = trues[:,:,-1]\n",
        "        r2 = metric2(preds, trues)\n",
        "        print('mse:{}, mae:{}, rmse:{}, mape:{}'.format(mse, mae, rmse, mape))\n",
        "        print('r2:{}'.format(r2))\n",
        "        np.save(folder_path+'metrics.npy', np.array([mae, mse, rmse, mape, mspe]))\n",
        "\n",
        "        return\n",
        "\n",
        "    def predict(self, setting, load=False):\n",
        "        pred_data, pred_loader = self._get_data(flag='pred')\n",
        "\n",
        "        if load:\n",
        "            path = os.path.join(self.args.checkpoints, setting)\n",
        "            best_model_path = path+'/'+'checkpoint.pth'\n",
        "            self.model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(pred_loader):\n",
        "            pred, true = self._process_one_batch(\n",
        "                pred_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
        "            preds.append(pred.detach().cpu().numpy())\n",
        "\n",
        "        preds = np.array(preds)\n",
        "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
        "\n",
        "        # result save\n",
        "        folder_path = './results/' + setting +'/'\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "        np.save(folder_path+'real_prediction.npy', preds)\n",
        "\n",
        "        return\n",
        "\n",
        "    def _process_one_batch(self, dataset_object, batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
        "        batch_x = batch_x.float().to(self.device)\n",
        "        batch_y = batch_y.float()\n",
        "\n",
        "        batch_x_mark = batch_x_mark.float().to(self.device)\n",
        "        batch_y_mark = batch_y_mark.float().to(self.device)\n",
        "\n",
        "        # decoder input\n",
        "        if self.args.padding==0:\n",
        "            dec_inp = torch.zeros([batch_y.shape[0], self.args.pred_len, batch_y.shape[-1]]).float()\n",
        "        elif self.args.padding==1:\n",
        "            dec_inp = torch.ones([batch_y.shape[0], self.args.pred_len, batch_y.shape[-1]]).float()\n",
        "        dec_inp = torch.cat([batch_y[:,:self.args.label_len,:], dec_inp], dim=1).float().to(self.device)\n",
        "        # encoder - decoder\n",
        "        if self.args.use_amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                if self.args.output_attention:\n",
        "                    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                else:\n",
        "                    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "        else:\n",
        "            if self.args.output_attention:\n",
        "                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "            else:\n",
        "                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "        if self.args.inverse:\n",
        "            outputs = dataset_object.inverse_transform(outputs)\n",
        "        f_dim = -1 if self.args.features=='MS' else 0\n",
        "        batch_y = batch_y[:,-self.args.pred_len:,f_dim:].to(self.device)\n",
        "\n",
        "        return outputs, batch_y\n"
      ],
      "metadata": {
        "id": "Q6L8IOaFXapA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if need stock data,use code can catch data"
      ],
      "metadata": {
        "id": "FdoxNzkYXaxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#import sys\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "import yfinance as yf\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "def get_stock_data(symbol, interval, api_key, month):\n",
        "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}&month={month}&outputsize=full&apikey={api_key}&datatype=csv\"\n",
        "    #url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}&datatype=csv\"\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.text.splitlines()\n",
        "        print(data)\n",
        "        return data\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: Failed to fetch data.\")\n",
        "        return None\n",
        "\n",
        "def convert_to_dataframe(raw_data):\n",
        "    header = raw_data[0].split(',')\n",
        "    data_rows = [row.split(',') for row in raw_data[1:]]\n",
        "\n",
        "    df = pd.DataFrame(data_rows, columns=header)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].apply(pd.to_numeric)\n",
        "\n",
        "    df.sort_values(by='timestamp', ascending=True, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_full_data(symbol, interval, api_key):\n",
        "    slices = ['2023-01']\n",
        "\n",
        "    print(slices)\n",
        "    all_data = []\n",
        "    for month in slices:\n",
        "        raw_stock_data = get_stock_data(symbol, interval, api_key, month)\n",
        "        if raw_stock_data:\n",
        "            all_data.extend(raw_stock_data[1:])\n",
        "        time.sleep(15)  #wait\n",
        "\n",
        "    all_data.insert(0, raw_stock_data[0])\n",
        "    stock_dataframe = convert_to_dataframe(all_data)\n",
        "\n",
        "    return stock_dataframe\n",
        "\n",
        "symbol = \"AAPL\" #Ticker\n",
        "interval = \"15min\"  #Interval\n",
        "api_key = \"***************\"  #Enter your API key obtained from Alpha Vantage here.\n",
        "\n",
        "stock_dataframe = get_full_data(symbol, interval, api_key)\n",
        "stock_dataframe.rename(columns={'timestamp': 'date'}, inplace=True)\n",
        "stock_dataframe.head()\n",
        "df = stock_dataframe\n",
        "\n",
        "output_directory = '/content/'\n",
        "csv_filename = \"output.csv\"\n",
        "output_path = os.path.join(output_directory, csv_filename)\n",
        "df.to_csv(output_path, index=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "hn82cTmFXa6g",
        "outputId": "430e0b55-5b4f-4566-cdc1-57955fe0d9e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2023-01']\n",
            "https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=AAPL&interval=15min&month=2023-01&outputsize=full&apikey=***************&datatype=csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-381c99baa3ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"***************\"\u001b[0m  \u001b[0;31m#Enter your API key obtained from Alpha Vantage here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mstock_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_full_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mstock_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mstock_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-381c99baa3ce>\u001b[0m in \u001b[0;36mget_full_data\u001b[0;34m(symbol, interval, api_key)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mraw_stock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stock_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw_stock_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stock_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-381c99baa3ce>\u001b[0m in \u001b[0;36mget_stock_data\u001b[0;34m(symbol, interval, api_key, month)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}&datatype=csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "upload DATA"
      ],
      "metadata": {
        "id": "AuyAJvdNz9KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "dX7y1Rxxz9U7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "6227d7a6-668d-4904-daa6-a75e8e21751f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7b07881-7e4b-4e14-8912-4d103f437c40\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e7b07881-7e4b-4e14-8912-4d103f437c40\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving output.csv to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA arrange\n",
        "\n"
      ],
      "metadata": {
        "id": "f79PdmIHXbCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IBM_path = 'output.csv'\n",
        "df = pd.read_csv(IBM_path, delimiter=',', usecols=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
        "df['volume'].replace(to_replace=0, method='ffill', inplace=True)\n",
        "df.sort_values('date', inplace=True)\n",
        "df.tail()\n",
        "IBM_path = 'output.csv'\n",
        "df = pd.read_csv(IBM_path, delimiter=',', usecols=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
        "df['volume'].replace(to_replace=0, method='ffill', inplace=True)\n",
        "df.sort_values('date', inplace=True)\n",
        "df.tail()\n",
        "\n",
        "output_directory = '/content/'\n",
        "csv_filename = \"output.csv\"\n",
        "output_path = os.path.join(output_directory, csv_filename)\n",
        "df.to_csv(output_path, index=True)\n",
        "df.head()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1ttnRWlhXbLq",
        "outputId": "9aec7460-81be-4938-fc68-fbe379a1b369"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                date     open     high      low    close  volume\n",
              "223  2023/1/10 04:45  133.089  133.123  133.074  133.104     200\n",
              "224  2023/1/10 06:15  133.186  133.221  133.171  133.201     450\n",
              "225  2023/1/10 07:00  132.895  132.929  132.734  132.763    2152\n",
              "226  2023/1/10 07:45  132.700  132.832  132.685  132.812     200\n",
              "227  2023/1/10 08:00  132.798  133.580  132.734  132.783    8448"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60b60729-6045-4238-957e-abc0e8e31948\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>2023/1/10 04:45</td>\n",
              "      <td>133.089</td>\n",
              "      <td>133.123</td>\n",
              "      <td>133.074</td>\n",
              "      <td>133.104</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>2023/1/10 06:15</td>\n",
              "      <td>133.186</td>\n",
              "      <td>133.221</td>\n",
              "      <td>133.171</td>\n",
              "      <td>133.201</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>2023/1/10 07:00</td>\n",
              "      <td>132.895</td>\n",
              "      <td>132.929</td>\n",
              "      <td>132.734</td>\n",
              "      <td>132.763</td>\n",
              "      <td>2152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>2023/1/10 07:45</td>\n",
              "      <td>132.700</td>\n",
              "      <td>132.832</td>\n",
              "      <td>132.685</td>\n",
              "      <td>132.812</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>2023/1/10 08:00</td>\n",
              "      <td>132.798</td>\n",
              "      <td>133.580</td>\n",
              "      <td>132.734</td>\n",
              "      <td>132.783</td>\n",
              "      <td>8448</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60b60729-6045-4238-957e-abc0e8e31948')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-60b60729-6045-4238-957e-abc0e8e31948 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-60b60729-6045-4238-957e-abc0e8e31948');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d57d3b6a-ee9f-46bf-b5ec-8a422612fa3a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d57d3b6a-ee9f-46bf-b5ec-8a422612fa3a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d57d3b6a-ee9f-46bf-b5ec-8a422612fa3a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14064,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 14064,\n        \"samples\": [\n          \"2023/7/27 10:15\",\n          \"2023/3/30 12:00\",\n          \"2023/8/30 06:15\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.018389506346567,\n        \"min\": 119.936,\n        \"max\": 169.586,\n        \"num_unique_values\": 6307,\n        \"samples\": [\n          143.199,\n          134.907,\n          148.059\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"high\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.010371625413303,\n        \"min\": 120.19,\n        \"max\": 169.97,\n        \"num_unique_values\": 6236,\n        \"samples\": [\n          144.932,\n          155.19,\n          127.075\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.024973439278426,\n        \"min\": 119.669,\n        \"max\": 169.467,\n        \"num_unique_values\": 6259,\n        \"samples\": [\n          167.131,\n          125.034,\n          143.678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.020947772233354,\n        \"min\": 119.92,\n        \"max\": 169.591,\n        \"num_unique_values\": 6502,\n        \"samples\": [\n          129.516,\n          139.532,\n          142.149\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 663047,\n        \"min\": 1,\n        \"max\": 23000218,\n        \"num_unique_values\": 9958,\n        \"samples\": [\n          1536,\n          513467,\n          342275\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "main import"
      ],
      "metadata": {
        "id": "3WZ4SleLXbUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = dotdict()\n",
        "args.model = 'informer'\n",
        "\n",
        "args.data = 'custom'\n",
        "args.root_path = '/content/'\n",
        "args.data_path = 'output.csv'\n",
        "args.features = 'MS'\n",
        "args.target = 'close'\n",
        "args.freq = '15m'\n",
        "args.checkpoints = './informer_checkpoints'\n",
        "args.seq_len = 96\n",
        "args.label_len = 48\n",
        "args.pred_len = 24\n",
        "args.enc_in = 8\n",
        "args.dec_in = 8\n",
        "args.c_out = 8\n",
        "args.factor = 5\n",
        "args.d_model = 512\n",
        "args.n_heads = 8\n",
        "args.e_layers = 3\n",
        "args.d_layers = 2\n",
        "args.d_ff = 2048\n",
        "args.dropout = 0.05\n",
        "args.attn = 'prob'\n",
        "args.embed = 'timeF'\n",
        "args.activation = 'gelu'\n",
        "args.distil = True\n",
        "args.output_attention = False\n",
        "args.mix = True\n",
        "args.padding = 0\n",
        "args.batch_size = 32\n",
        "args.learning_rate = 0.0001\n",
        "args.loss = 'mse+Charbonnier'\n",
        "args.lradj = 'type1'\n",
        "args.use_amp = False\n",
        "args.num_workers = 0\n",
        "args.itr = 1\n",
        "args.train_epochs = 20\n",
        "args.patience = 3\n",
        "#args.des = 'test'\n",
        "args.des = 'exp'\n",
        "args.use_gpu = True if torch.cuda.is_available() else False\n",
        "args.gpu = 0\n",
        "args.use_multi_gpu = False\n",
        "args.devices = '0,1,2,3'"
      ],
      "metadata": {
        "id": "RLeiWakGXbb4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第十七步驟:"
      ],
      "metadata": {
        "id": "7BjbXDbZXbkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(' ','')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]"
      ],
      "metadata": {
        "id": "yz3INuT6zbfT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第十八步驟:"
      ],
      "metadata": {
        "id": "uDtsTGXXzbuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_parser = {\n",
        "    'custom':{'data':'output.csv','T':'close','M':[6,6,6],'S':[1,1,1],'MS':[6,6,1]},\n",
        "}\n",
        "if args.data in data_parser.keys():\n",
        "    data_info = data_parser[args.data]\n",
        "    args.data_path = data_info['data']\n",
        "    args.target = data_info['T']\n",
        "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]\n",
        "\n",
        "\n",
        "args.detail_freq = args.freq\n",
        "args.freq = args.freq[-1:]\n",
        "\n",
        "Exp = Exp_Informer\n",
        "\n",
        "print('Args in experiment:')\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuJe63pxzb3z",
        "outputId": "92974d1e-ba37-41dc-8376-dd3e1372aa61"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args in experiment:\n",
            "{'model': 'informer', 'data': 'custom', 'root_path': '/content/', 'data_path': 'output.csv', 'features': 'MS', 'target': 'close', 'freq': 'm', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 6, 'dec_in': 6, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 3, 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse+Charbonnier', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 20, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': '15m'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第十九步驟: 訓練"
      ],
      "metadata": {
        "id": "OXhN8PEpzcCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ii in range(args.itr):\n",
        "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features,\n",
        "                args.seq_len, args.label_len, args.pred_len,\n",
        "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
        "\n",
        "\n",
        "    exp = Exp(args)\n",
        "\n",
        "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
        "    exp.train(setting)\n",
        "\n",
        "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "    exp.test(setting)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAJCInZMzcMc",
        "outputId": "96b617c5-6357-4916-b6a3-2bb01c82b497"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : informer_custom_ftMS_sl96_ll48_pl24_dm512_nh8_el3_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 9725\n",
            "val 1385\n",
            "test 2789\n",
            "\titers: 100, epoch: 1 | loss: 0.1403116\n",
            "\tspeed: 0.2295s/iter; left time: 1368.0042s\n",
            "\titers: 200, epoch: 1 | loss: 0.1252163\n",
            "\tspeed: 0.1719s/iter; left time: 1007.3642s\n",
            "\titers: 300, epoch: 1 | loss: 0.1514652\n",
            "\tspeed: 0.1752s/iter; left time: 1009.4185s\n",
            "Epoch: 1 cost time: 54.29597878456116\n",
            "Epoch: 1, Steps: 303 | Train Loss: 0.2437998 Vali Loss: 0.1077948 Test Loss: 0.1317195\n",
            "Validation loss decreased (inf --> 0.107795).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.1394453\n",
            "\tspeed: 0.2627s/iter; left time: 1486.3207s\n",
            "\titers: 200, epoch: 2 | loss: 0.1109120\n",
            "\tspeed: 0.1820s/iter; left time: 1011.5227s\n",
            "\titers: 300, epoch: 2 | loss: 0.1986057\n",
            "\tspeed: 0.1795s/iter; left time: 979.8237s\n",
            "Epoch: 2 cost time: 54.77371311187744\n",
            "Epoch: 2, Steps: 303 | Train Loss: 0.1324265 Vali Loss: 0.1296192 Test Loss: 0.1392525\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1006171\n",
            "\tspeed: 0.2590s/iter; left time: 1386.8105s\n",
            "\titers: 200, epoch: 3 | loss: 0.1017632\n",
            "\tspeed: 0.1791s/iter; left time: 941.1171s\n",
            "\titers: 300, epoch: 3 | loss: 0.0696630\n",
            "\tspeed: 0.1797s/iter; left time: 926.4574s\n",
            "Epoch: 3 cost time: 54.247955322265625\n",
            "Epoch: 3, Steps: 303 | Train Loss: 0.1058021 Vali Loss: 0.1238050 Test Loss: 0.1235960\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.0800495\n",
            "\tspeed: 0.2605s/iter; left time: 1315.8977s\n",
            "\titers: 200, epoch: 4 | loss: 0.0874471\n",
            "\tspeed: 0.1790s/iter; left time: 886.2961s\n",
            "\titers: 300, epoch: 4 | loss: 0.0890156\n",
            "\tspeed: 0.1788s/iter; left time: 867.7226s\n",
            "Epoch: 4 cost time: 54.2779905796051\n",
            "Epoch: 4, Steps: 303 | Train Loss: 0.0923302 Vali Loss: 0.1228283 Test Loss: 0.1023344\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : informer_custom_ftMS_sl96_ll48_pl24_dm512_nh8_el3_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 2789\n",
            "test shape: (87, 32, 24, 1) (87, 32, 24, 1)\n",
            "test shape: (2784, 24, 1) (2784, 24, 1)\n",
            "mse:0.05050981789827347, mae:0.15213708579540253, rmse:0.22474388778209686, mape:0.4447071850299835\n",
            "r2:0.6148807406425476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第二十步驟:"
      ],
      "metadata": {
        "id": "TrFmSAv-zcib"
      }
    }
  ]
}